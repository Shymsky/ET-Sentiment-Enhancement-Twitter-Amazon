{"cells":[{"cell_type":"markdown","metadata":{"id":"zDUGvy8NQVh7"},"source":["#**2a_Find BERT candidates Twitter & Amazon data set**\n","This notebook is used to find the slang word candidates for each of the product review/tweet, which is later (see notebook 2b) passed to BERT that will decide whether to replace the slang word."]},{"cell_type":"markdown","metadata":{"id":"f7cBNY65QViB"},"source":["## Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CchYXmzxQViF"},"outputs":[],"source":["import pandas as pd\n","import re\n","from tqdm import tqdm\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"I0WQWMF0QViG"},"source":["## Parameters for the notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfQPgioJQViG"},"outputs":[],"source":["pd.set_option('display.max_colwidth', None)\n","# TO RUN LOCALLY OR ON COLAB\n","# is_local = True\n","is_local = False\n","\n","# TO USE SMALL SLANG OR BIGGER ONE\n","use_small_slang = True\n","# use_small_slang = False"]},{"cell_type":"markdown","metadata":{"id":"6BLlq4a8QViH"},"source":["## Reading in cleaned data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15738,"status":"ok","timestamp":1698328072233,"user":{"displayName":"Szymski","userId":"12735740458565246672"},"user_tz":-120},"id":"mdQa0hdgQViI","outputId":"7c38b67c-af25-46fb-c978-78ae352dc099"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","df_slang_small shape: (159, 2)\n","df_slang_big shape: (2317, 2)\n","df_amazon shape: (393568, 3)\n","df_twitter shape: (1581466, 3)\n"]}],"source":["if is_local == False:\n","    from google.colab import drive\n","    drive.mount('/content/drive/')\n","    path_data = \"/content/drive/My Drive/NLP_PROJECT/data/\"\n","else:\n","    path_data = \"D:/Google Drive/NLP_PROJECT/data/\"\n","\n","# slang\n","df_slang_small = pd.read_csv(path_data + \"Slang/m_slang_small_cleaned.csv\")\n","df_slang_big = pd.read_csv(path_data + \"Slang/m_slang_big_cleaned.csv\")\n","# datasets\n","df_amazon = pd.read_csv(path_data + \"Amazon/amazon_cleaned.csv\")\n","df_twitter = pd.read_csv(path_data + \"Twitter/twitter_cleaned.csv\")\n","# shapes\n","print(\"df_slang_small shape:\", df_slang_small.shape)\n","print(\"df_slang_big shape:\", df_slang_big.shape)\n","print(\"df_amazon shape:\", df_amazon.shape)\n","print(\"df_twitter shape:\", df_twitter.shape)"]},{"cell_type":"markdown","metadata":{"id":"ele82s2FQViJ"},"source":["## Finding the slang word candidates among the words in tweets/product reviews\n","\n","The purpose of the function **find_slang_for_bert** is to identify the occurences of slang words in the input text, provide the translations for these slang words, and enrichen the candidates with further information that can be used for our processing in notebook 2b, which may replace slang words with their translations based on the probability which word(s) would suit the best, either the slang/chat word or the translation. **We will give you an example for what happens in Notebook 2b to reduce the level of complexity.** Consider the following text and slang/chatword-translation pair:\n","\n","- **\"it has been a great day\"**\n","- {it: \"information technology\"}\n","\n","We cannot say we want to replace slang/chat words every time, as it heavily depends on the context. For most cases, we assume that \"it\" must remain \"it\" as a pronoun. However, \"it\" refers to \"information technology\" when the context of the text is within the domain of information technology. Let's now see how that goes in Jupyter Notebook 2b.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqGbERWYQViK"},"outputs":[],"source":["# precompiled regex patterns\n","if use_small_slang:\n","    df_slang = df_slang_small\n","    pattern_dict = {slang_word: re.compile(r'\\b' + re.escape(str(slang_word)) + r'\\b') for slang_word in set(df_slang_small['slang'].tolist())}\n","else:\n","    df_slang = df_slang_big\n","    pattern_dict = {slang_word: re.compile(r'\\b' + re.escape(str(slang_word)) + r'\\b') for slang_word in set(df_slang_big['slang'].tolist())}\n","\n","# columns for dataframe for the results\n","list_of_columns = ['id', 'true_sentiment', 'text', 'candidates']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jUB0OEiqQViK"},"outputs":[],"source":["def find_slang_for_bert(text, df_slang, pattern_dict):\n","    # print(\"------------------------------------------------------------------\\n\", \"FULL text: \", text)\n","    list_of_candidates_for_text = []\n","    # COMMENT - look at each slang word and its possible translations\n","    for slang_word, pattern in pattern_dict.items():\n","        # find all occurences of slang_word in text\n","        match_indices = [(m.start(0), m.end(0)) for m in re.finditer(pattern, text)]\n","        if match_indices:\n","            # print(\"\\tslang_word: \", slang_word, \"\\n\\tpattern: \", pattern, \"\\n\\tmatch_indices: \", match_indices)\n","            # COMMENT - for each occurence of THIS slang_word in text process it\n","            # find all possible translation for the slang_word from df\n","            list_of_translations = [slang_word] + df_slang[df_slang['slang'] == slang_word]['translation'].to_list()\n","            for matched_start_index, matched_end_index in match_indices:\n","                # create a dict for each occurence of slang_word\n","                candidate = {\n","                                \"slang_word\": slang_word,\n","                                \"start_index\": matched_start_index,\n","                                \"end_index\": matched_end_index,\n","                                \"list_of_translations\": list_of_translations,\n","                                \"text_to_check\": text[:matched_start_index] + '___' + text[matched_end_index:],\n","                                \"chosen_translation\": None,\n","                                \"was_replaced\": False,\n","                                \"final_text\": None,\n","                                \"diff\": None\n","                            }\n","                list_of_candidates_for_text.append(candidate)\n","            #     print(\"\\tcandidate: \", candidate)\n","            # print(\"\\t------------------------------------------------------------------\\n\")\n","    # sort the list of candidates by start_index\n","    list_of_candidates_for_text = sorted(list_of_candidates_for_text, key=lambda x: x[\"start_index\"])\n","    # return\n","    return list_of_candidates_for_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Au1eVD6sZdhR"},"outputs":[],"source":["# # execution for amazon\n","# df_amazon_for_bert = pd.DataFrame(columns=list_of_columns)\n","# dict_amazon_for_bert = {}\n","# i = 0\n","# with tqdm(total=len(df_amazon)) as pbar:\n","#     for text, sentiment in zip(df_amazon['Text'], df_amazon['Sentiment']):\n","#         candidates = find_slang_for_bert(text, df_slang, pattern_dict)\n","#         df_amazon_for_bert.loc[len(df_amazon_for_bert)] = {\n","#             'id': i,\n","#             'true_sentiment': sentiment,\n","#             'text': text,\n","#             'candidates': candidates\n","#         }\n","#         dict_amazon_for_bert[i] = {\n","#             'true_sentiment': sentiment,\n","#             'text': text,\n","#             'candidates': candidates\n","#         }\n","#         pbar.update(1)\n","#         i += 1\n","\n","# # save to csv and dict\n","# if use_small_slang:\n","#     df_amazon_for_bert.to_csv(path_data + \"BERT_data/amazon_for_bert_small.csv\", index=False)\n","#     with open(path_data + \"BERT_data/amazon_for_bert_small.pkl\", 'wb') as f:\n","#         pickle.dump(dict_amazon_for_bert, f)\n","# else:\n","#     df_amazon_for_bert.to_csv(path_data + \"BERT_data/amazon_for_bert_big.csv\", index=False)\n","#     with open(path_data + \"BERT_data/amazon_for_bert_big.pkl\", 'wb') as f:\n","#         pickle.dump(dict_amazon_for_bert, f)"]},{"cell_type":"markdown","source":["## Limit the number of rows for the data set\n","\n","Due to the huge size of this dataset, we reduce while still considering its underlying structure and distribution of sentiments (binary target values for Twitter or scores for Amazon dataset)."],"metadata":{"id":"o9i6jQTFT021"}},{"cell_type":"code","source":["df_twitter = pd.concat([df_twitter.head(200000), df_twitter.tail(200000)])\n","df_twitter = df_twitter.reset_index(drop=True)"],"metadata":{"id":"P-Fdsts5T1EB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HN0f5mWKZdhR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698334469614,"user_tz":-120,"elapsed":6336456,"user":{"displayName":"Szymski","userId":"12735740458565246672"}},"outputId":"331df3f0-c877-4495-b10c-2fe743f08bce"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 400000/400000 [1:45:32<00:00, 63.17it/s]\n"]}],"source":["# execution for twitter\n","df_twitter_for_bert = pd.DataFrame(columns=list_of_columns)\n","dict_twitter_for_bert = {}\n","i = 0\n","with tqdm(total=len(df_twitter)) as pbar:\n","    for text, sentiment in zip(df_twitter['Text'], df_twitter['Sentiment']):\n","        candidates = find_slang_for_bert(str(text), df_slang, pattern_dict)\n","        df_twitter_for_bert.loc[len(df_twitter_for_bert)] = {\n","            'id': i,\n","            'true_sentiment': sentiment,\n","            'text': text,\n","            'candidates': candidates\n","        }\n","        dict_twitter_for_bert[i] = {\n","            'true_sentiment': sentiment,\n","            'text': text,\n","            'candidates': candidates\n","        }\n","        pbar.update(1)\n","        i += 1\n","\n","# save to csv and dict\n","if use_small_slang:\n","    df_twitter_for_bert.to_csv(path_data + \"BERT_data/twitter_for_bert_small_400k.csv\", index=False)\n","    with open(path_data + \"BERT_data/twitter_for_bert_small_400k.pkl\", 'wb') as f:\n","        pickle.dump(dict_twitter_for_bert, f)\n","else:\n","    df_twitter_for_bert.to_csv(path_data + \"BERT_data/twitter_for_bert_big_400k.csv\", index=False)\n","    with open(path_data + \"BERT_data/twitter_for_bert_big_400k.pkl\", 'wb') as f:\n","        pickle.dump(dict_twitter_for_bert, f)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}