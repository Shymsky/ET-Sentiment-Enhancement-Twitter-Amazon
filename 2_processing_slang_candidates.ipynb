{"cells":[{"cell_type":"markdown","metadata":{"id":"zDUGvy8NQVh7"},"source":["This notebook is used to replace the slang word."]},{"cell_type":"markdown","source":["#**2_Preprocessing Slang Candidates**"],"metadata":{"id":"u3TQzg76Dqzu"}},{"cell_type":"markdown","metadata":{"id":"f7cBNY65QViB"},"source":["## Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CchYXmzxQViF"},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"I0WQWMF0QViG"},"source":["## Parameters for the notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfQPgioJQViG"},"outputs":[],"source":["pd.set_option('display.max_colwidth', None)\n","# TO RUN LOCALLY OR ON COLAB\n","is_local = True"]},{"cell_type":"markdown","metadata":{"id":"6BLlq4a8QViH"},"source":["## Reading in cleaned data sets (Twitter & Amazon product review)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdQa0hdgQViI"},"outputs":[],"source":["if is_local == False:\n","    from google.colab import drive\n","    drive.mount('/content/drive/')\n","    path_data = \"/content/drive/My Drive/NLP_PROJECT/data/\"\n","else:\n","    path_data = \"D:/Google Drive/NLP_PROJECT/data/\"\n","\n","with open(path_data + 'BERT_data/amazon_for_bert_small.pkl', 'rb') as handle:\n","    dict_amazon_for_bert = pickle.load(handle)\n","with open(path_data + 'BERT_data/twitter_for_bert_small_400k.pkl', 'rb') as handle:\n","    dict_twitter_for_bert = pickle.load(handle)"]},{"cell_type":"markdown","source":["## Preprocessing\n","\n","We only want to keep tweets/product reviews which actually have at least one slang/chatword. Consequently, text without any slang/chatwords are dropped.\n","\n","As we can see from the output of the following code cell, we only took a portion of the initial data set, due to their extreme size. Regardless, we have nearly 130k texts for the Amazon dataset, that include at least one word that might want to be replaced by its translation. For the Twitter dataset, we are talking about more than 70k Texts that consists of at least one replacement candidate"],"metadata":{"id":"QDuDkZGqEU_0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6AGHvGwyb2ll","outputId":"e43765fb-bbf7-4d64-f2b5-cd4c174f354e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of amazon before filtering:  393568\n","Length of amazon after filtering:  128077\n","Length of twitter before filtering:  400000\n","Length of twitter after filtering:  75312\n"]}],"source":["# keep only tweets/reviews that have some candidates detected\n","print(\"Length of amazon before filtering: \", len(dict_amazon_for_bert))\n","dict_amazon_for_bert = {key: value for key, value in dict_amazon_for_bert.items() if len(value['candidates']) > 0}\n","print(\"Length of amazon after filtering: \", len(dict_amazon_for_bert))\n","\n","print(\"Length of twitter before filtering: \", len(dict_twitter_for_bert))\n","dict_twitter_for_bert = {key: value for key, value in dict_twitter_for_bert.items() if len(value['candidates']) > 0}\n","print(\"Length of twitter after filtering: \", len(dict_twitter_for_bert))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ostD0noCDc3P"},"outputs":[],"source":["# def limit_dict_size(dictionary, limit):\n","#     return {k: dictionary[k] for k in list(dictionary.keys())[:limit]}\n","\n","def check_dataset_distribution(dataset, proportion):\n","    df = pd.DataFrame.from_dict(dataset, orient='index')\n","    unique_counts = df['true_sentiment'].value_counts()\n","    print(\"Distribution of values: \\n\", unique_counts)\n","    proportion\n","    print(\"\\nTotal count: \", round(sum(unique_counts * proportion), 0))\n","    max_counts = list((round(unique_counts * proportion, 0)))\n","    print(\"\\nModified distribution of values: \\n\", max_counts)\n","    return max_counts\n","\n","def limit_dataset(dictionary, m_counts, use_amazon):\n","    copied_dict = {}\n","    if use_amazon:\n","      sentiment_counts = {5: m_counts[0], 4: m_counts[1], 3: m_counts[2], 2: m_counts[3], 1: m_counts[4]}\n","    else:\n","      sentiment_counts = {1: m_counts[0], 0: m_counts[1]}\n","    for key, value in dictionary.items():\n","        value_sentiment = value['true_sentiment']\n","        if value_sentiment in sentiment_counts and sentiment_counts[value_sentiment] > 0:\n","            copied_dict[key] = value\n","            sentiment_counts[value_sentiment] -= 1\n","        if all(count == 0 for count in sentiment_counts.values()):\n","            break\n","    return copied_dict\n","\n","def process_dataset(dataset, max_distribution, use_amazon=False):\n","    max_counts = check_dataset_distribution(dataset, max_distribution)\n","    print(\"\\n-------------------------------------------\")\n","    dataset = limit_dataset(dataset, max_counts, use_amazon)\n","    print(\"\\nTotal count: \", len(dataset))\n","    df = pd.DataFrame.from_dict(dataset, orient='index')\n","    unique_counts = df['true_sentiment'].value_counts()\n","    print(\"\\nModified distribution of values: \\n\", unique_counts)\n","    return dataset"]},{"cell_type":"markdown","source":["**Here we shrink the dataset even further, as we can only handle around 20k rows within a reasonable time frame**"],"metadata":{"id":"7IzSxPQNFtt3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfGaTP5IDc3Q","outputId":"e75bec88-ff5f-496b-f10d-31a420f91ca1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Distribution of values: \n"," 5    80283\n","4    19377\n","1    11211\n","3    10317\n","2     6889\n","Name: true_sentiment, dtype: int64\n","\n","Total count:  19212.0\n","\n","Modified distribution of values: \n"," [12042.0, 2907.0, 1682.0, 1548.0, 1033.0]\n","\n","-------------------------------------------\n","\n","Total count:  19212\n","\n","Modified distribution of values: \n"," 5    12042\n","4     2907\n","3     1682\n","2     1548\n","1     1033\n","Name: true_sentiment, dtype: int64\n"]}],"source":["# Process the Amazon dataset\n","dict_amazon_for_bert = process_dataset(dict_amazon_for_bert, 0.15, use_amazon=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePOjs4IkDc3Q","outputId":"7eca2590-f41a-48df-f4c0-b26eb1f343dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Distribution of values: \n"," 0    37977\n","1    37335\n","Name: true_sentiment, dtype: int64\n","\n","Total count:  18828.0\n","\n","Modified distribution of values: \n"," [9494.0, 9334.0]\n","\n","-------------------------------------------\n","\n","Total count:  18828\n","\n","Modified distribution of values: \n"," 1    9494\n","0    9334\n","Name: true_sentiment, dtype: int64\n"]}],"source":["# Process the Twitter dataset\n","dict_twitter_for_bert = process_dataset(dict_twitter_for_bert, 0.25, use_amazon=False)"]},{"cell_type":"markdown","metadata":{"id":"y0XCY6DKQViO"},"source":["## Processing of candidates with a script\n","The following code checks replaces all the slang/chatword candidates with \"__\" while keeping track of the index to then replace them with the corresponding translation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9yiYJ089QViO"},"outputs":[],"source":["def replace_slang(dict_text_for_bert, use_progressive_updated_text):\n","    df_text_after = pd.DataFrame(columns=['id','true_sentiment', 'text', 'candidates',\n","                                               'updated_candidates', 'processed_text',\n","                                               'is_same_as_original', 'chosen_translation'])\n","    dict_text_after = {}\n","\n","    with tqdm(total=len(dict_text_for_bert)) as pbar:\n","        for index, row in dict_text_for_bert.items():\n","            text = row['text']\n","            index_shift = 0\n","            list_of_candidates_for_text = row['candidates'] # list of dicts\n","            updated_list_of_candidates_for_text = []\n","            updated_text_to_check = ''\n","            # print(\"------------------------------------------------------------------\\n\", \"FULL text: \", text)\n","            # process the candidates in order that was sorted by start_index\n","            for candidate in list_of_candidates_for_text:\n","                # print(\"\\tcandidate: \", candidate)\n","\n","                if updated_text_to_check != '' and use_progressive_updated_text == True:\n","                    # temp = updated_text_to_check\n","                    updated_text_to_check = updated_text_to_check[:candidate['start_index']-index_shift] + '___' + updated_text_to_check[candidate['end_index']-index_shift:]\n","                    # if candidate['text_to_check'] != updated_text_to_check:\n","                    #     print('\\index_shift: ' + str(index_shift))\n","                    #     print('\\tupdated_text_to_check: ' + temp)\n","                    #     print(\"\\tCandidate OLD text_to_check: \", candidate['text_to_check'])\n","                    #     print(\"\\tCandidate NEW text_to_check: \", updated_text_to_check)\n","                    candidate['text_to_check'] = updated_text_to_check\n","\n","                my_dict = {'slang': candidate['slang_word'],\n","                            'candidates': candidate['list_of_translations'],\n","                            'text_to_check': candidate['text_to_check']}\n","                predicted_word = my_dict['candidates'][1]\n","\n","                # update text and index_shift since start_index and end_index is always wrt to original text\n","                text = text[:candidate['start_index']-index_shift] + predicted_word + text[candidate['end_index']-index_shift:]\n","                # as long as slang_word == predicted_word, the index_shift will be the same\n","                index_shift = index_shift + len(candidate['slang_word']) - len(predicted_word)\n","                updated_text_to_check = text\n","\n","                # update candidate\n","                difference = ''\n","                if predicted_word == candidate['slang_word']:\n","                    processed_candidate =  candidate.copy()\n","                    processed_candidate['chosen_translation'] = candidate['slang_word']\n","                    processed_candidate['was_replaced'] = False\n","                    processed_candidate['final_text'] = my_dict['text_to_check'].replace('___', candidate['slang_word'])\n","                    processed_candidate['diff'] = difference\n","                else:\n","                    processed_candidate = candidate.copy()\n","                    processed_candidate['chosen_translation'] = predicted_word\n","                    processed_candidate['was_replaced'] = True\n","                    processed_candidate['final_text'] = my_dict['text_to_check'].replace('___', predicted_word)\n","                    processed_candidate['diff'] = difference\n","                updated_list_of_candidates_for_text.append(processed_candidate)\n","\n","            # append a row to df\n","            replacement_tuples = [(candidate['slang_word'], candidate['chosen_translation'], candidate['diff']) for candidate in updated_list_of_candidates_for_text if candidate['was_replaced']]\n","            df_text_after.loc[len(df_text_after)] = {\n","                                                                'id': index,\n","                                                                'true_sentiment': row['true_sentiment'],\n","                                                                'text': row['text'],\n","                                                                'candidates': row['candidates'],\n","                                                                'updated_candidates': updated_list_of_candidates_for_text,\n","                                                                'processed_text': text,\n","                                                                'is_same_as_original': text == row['text'],\n","                                                                'chosen_translation': replacement_tuples\n","                                                                }\n","            dict_text_after[index] = {\n","                'true_sentiment': row['true_sentiment'],\n","                'text': row['text'],\n","                'candidates': row['candidates'],\n","                'updated_candidates': updated_list_of_candidates_for_text,\n","                'processed_text': text,\n","                'is_same_as_original': text == row['text'],\n","                'chosen_translation': replacement_tuples\n","\n","            }\n","            pbar.update(1)\n","\n","    return df_text_after, dict_text_after"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lA9h8zt9dTfB","outputId":"cdccdcb9-03eb-44bb-c654-56d5adcac014"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 19212/19212 [01:33<00:00, 204.92it/s]\n"]}],"source":["# execution for amazon\n","df_amazon_after, dict_amazon_after = replace_slang(dict_amazon_for_bert, use_progressive_updated_text = False)\n","\n","# save df and dict\n","file_name = 'amazon_after_script_small'\n","df_amazon_after.to_csv(path_data + 'BERT_data/' + file_name + '.csv', index=False)\n","with open(path_data + 'BERT_data/' + file_name + '.pkl', 'wb') as handle:\n","    pickle.dump(dict_amazon_after, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AuWJmv4ZdTfC","outputId":"7d5c02ac-8c2e-4790-b9fe-f4c46c9c4b39"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 18828/18828 [01:40<00:00, 187.77it/s]\n"]}],"source":["# # execution for twitter\n","df_twitter_after, dict_twitter_after = replace_slang(dict_twitter_for_bert, use_progressive_updated_text = False)\n","\n","# save df and dict\n","file_name = 'twitter_after_script_small'\n","df_twitter_after.to_csv(path_data + 'BERT_data/' + file_name + '.csv', index=False)\n","with open(path_data + 'BERT_data/' + file_name + '.pkl', 'wb') as handle:\n","    pickle.dump(dict_twitter_after, handle, protocol=pickle.HIGHEST_PROTOCOL)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}