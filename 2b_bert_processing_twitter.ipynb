{"cells":[{"cell_type":"markdown","metadata":{"id":"zDUGvy8NQVh7"},"source":["#**2b_BERT candidates replacement Twitter/Amazon**\n","\n","This notebook is used for BERT that will decide whether to replace the slang word.\n","- https://www.kaggle.com/code/tientd95/bert-model-for-anwsering-toeic-reading-test/notebook\n","- https://github.com/graykode/toeicbert/tree/master"]},{"cell_type":"markdown","metadata":{"id":"f7cBNY65QViB"},"source":["## Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37309,"status":"ok","timestamp":1698354799289,"user":{"displayName":"Szymski","userId":"12735740458565246672"},"user_tz":-120},"id":"HiEBHyNGQViC","outputId":"46a9bb7e-c51d-4602-911e-e60957928b5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers==4.28.0\n","  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.0)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.66.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: tokenizers, dill, multiprocess, huggingface-hub, transformers, datasets\n","Successfully installed datasets-2.14.6 dill-0.3.7 huggingface-hub-0.18.0 multiprocess-0.70.15 tokenizers-0.13.3 transformers-4.28.0\n","Collecting accelerate\n","  Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.18.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.24.0\n","Collecting cchardet\n","  Downloading cchardet-2.1.7.tar.gz (653 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.6/653.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: cchardet\n","  Building wheel for cchardet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cchardet: filename=cchardet-2.1.7-cp310-cp310-linux_x86_64.whl size=289191 sha256=32883adf47932732af37b9cbed1042d7a16e91c72c9d1c6cf10e74b8c7e6ba1d\n","  Stored in directory: /root/.cache/pip/wheels/ee/e0/ab/e01326f15c59438d080b1496dbab8091e952ec72f35e3c437e\n","Successfully built cchardet\n","Installing collected packages: cchardet\n","Successfully installed cchardet-2.1.7\n","Collecting pytorch-pretrained-bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.1.0+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.23.5)\n","Collecting boto3 (from pytorch-pretrained-bert)\n","  Downloading boto3-1.28.72-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (4.66.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2023.6.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2.1.0)\n","Collecting botocore<1.32.0,>=1.31.72 (from boto3->pytorch-pretrained-bert)\n","  Downloading botocore-1.31.72-py3-none-any.whl (11.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-pretrained-bert)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.8.0,>=0.7.0 (from boto3->pytorch-pretrained-bert)\n","  Downloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2023.7.22)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.72->boto3->pytorch-pretrained-bert) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.72->boto3->pytorch-pretrained-bert) (1.16.0)\n","Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","Successfully installed boto3-1.28.72 botocore-1.31.72 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.7.0\n"]}],"source":["!pip install datasets transformers==4.28.0\n","!pip install --upgrade accelerate\n","!pip install cchardet\n","!pip install -U pytorch-pretrained-bert;"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"CchYXmzxQViF","executionInfo":{"status":"ok","timestamp":1698354803313,"user_tz":-120,"elapsed":4028,"user":{"displayName":"Szymski","userId":"12735740458565246672"}}},"outputs":[],"source":["import pandas as pd\n","import torch\n","from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n","from tqdm import tqdm\n","import pickle\n","import copy"]},{"cell_type":"markdown","metadata":{"id":"I0WQWMF0QViG"},"source":["## Parameters for the notebook"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"LfQPgioJQViG","executionInfo":{"status":"ok","timestamp":1698354803314,"user_tz":-120,"elapsed":7,"user":{"displayName":"Szymski","userId":"12735740458565246672"}}},"outputs":[],"source":["pd.set_option('display.max_colwidth', None)\n","# TO RUN LOCALLY OR ON COLAB\n","# is_local = True\n","is_local = False\n","\n","# TO USE SMALL SLANG OR BIGGER ONE\n","use_small_slang = True\n","# use_small_slang = False\n","\n","# TO SET UP BERT CLASS TO USE PROGRESSIVE APPROACH FOR SLANG REPLACEMENT\n","use_bert_progressively = False\n","# use_bert_progressively = True\n","'''\n","use_bert_progressively\n","\n","    If set to True then BERT class will incorporate its previous decision on\n","    slang replacement for the next candidate replacement.\n","    That is if we have a text \"I luv u\" and BERT replaces \"luv\" with \"love\" then\n","    to evaluate whether 'u' should be replaced as well BERT will use \"I love u\" and not \"I luv u\" as a context.\n","'''\n","\n","def get_file_name(dataset_name):\n","    file_prefix = f'{dataset_name}_after_bert_small' if use_small_slang else f'{dataset_name}_after_bert_big'\n","    file_suffix = \"_progressive_local\" if use_bert_progressively and is_local else \"_progressive_colab\" if use_bert_progressively else \"_local\" if is_local else \"_colab\"\n","    file_name = file_prefix + file_suffix\n","    return file_name"]},{"cell_type":"markdown","metadata":{"id":"6BLlq4a8QViH"},"source":["## Reading in cleaned data"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"mdQa0hdgQViI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698355276329,"user_tz":-120,"elapsed":473022,"user":{"displayName":"Szymski","userId":"12735740458565246672"}},"outputId":"864d3caa-3b01-46e9-978d-2330d60ad753"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["if is_local == False:\n","    from google.colab import drive\n","    drive.mount('/content/drive/')\n","    path_data = \"/content/drive/My Drive/NLP_PROJECT/data/\"\n","else:\n","    path_data = \"D:/Google Drive/NLP_PROJECT/data/\"\n","\n","# datasets processed for bert\n","if use_small_slang:\n","    with open(path_data + 'BERT_data/amazon_for_bert_small.pkl', 'rb') as handle:\n","        dict_amazon_for_bert = pickle.load(handle)\n","    with open(path_data + 'BERT_data/twitter_for_bert_small_400k.pkl', 'rb') as handle:\n","        dict_twitter_for_bert = pickle.load(handle)\n","else:\n","    with open(path_data + 'BERT_data/amazon_for_bert_big.pkl', 'rb') as handle:\n","        dict_amazon_for_bert = pickle.load(handle)\n","    with open(path_data + 'BERT_data/twitter_for_bert_big.pkl', 'rb') as handle:\n","        dict_twitter_for_bert = pickle.load(handle)"]},{"cell_type":"markdown","source":["## Limit the number of rows for the data set\n","We are solely interested in texts with replacement words in it. Therefore, we remove texts without any slang/chat word candidates in it. As the data set is still way too large for our computation power to process this amount of data, we reduce the dataset even further, while still considering its underlying structure and distribution of sentiments (binary target values for Twitter or scores for Amazon dataset)."],"metadata":{"id":"BXe4jA-BTG-u"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"6AGHvGwyb2ll","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698355278128,"user_tz":-120,"elapsed":9,"user":{"displayName":"Szymski","userId":"12735740458565246672"}},"outputId":"08673968-5e0c-4f5b-86e3-08a3cf5210f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Length of amazon before filtering:  393568\n","Length of amazon after filtering:  128077\n","Length of twitter before filtering:  400000\n","Length of twitter after filtering:  75312\n"]}],"source":["# keep only tweets/reviews that have some candidates detected\n","print(\"Length of amazon before filtering: \", len(dict_amazon_for_bert))\n","dict_amazon_for_bert = {key: value for key, value in dict_amazon_for_bert.items() if len(value['candidates']) > 0}\n","print(\"Length of amazon after filtering: \", len(dict_amazon_for_bert))\n","\n","print(\"Length of twitter before filtering: \", len(dict_twitter_for_bert))\n","dict_twitter_for_bert = {key: value for key, value in dict_twitter_for_bert.items() if len(value['candidates']) > 0}\n","print(\"Length of twitter after filtering: \", len(dict_twitter_for_bert))"]},{"cell_type":"code","source":["# def limit_dict_size(dictionary, limit):\n","#     return {k: dictionary[k] for k in list(dictionary.keys())[:limit]}\n","\n","def check_dataset_distribution(dataset, proportion):\n","    df = pd.DataFrame.from_dict(dataset, orient='index')\n","    unique_counts = df['true_sentiment'].value_counts()\n","    print(\"Distribution of values: \\n\", unique_counts)\n","    proportion\n","    print(\"\\nTotal count: \", round(sum(unique_counts * proportion), 0))\n","    max_counts = list((round(unique_counts * proportion, 0)))\n","    print(\"\\nModified distribution of values: \\n\", max_counts)\n","    return max_counts\n","\n","def limit_dataset(dictionary, m_counts, use_amazon):\n","    copied_dict = {}\n","    if use_amazon:\n","      sentiment_counts = {5: m_counts[0], 4: m_counts[1], 3: m_counts[2], 2: m_counts[3], 1: m_counts[4]}\n","    else:\n","      sentiment_counts = {1: m_counts[0], 0: m_counts[1]}\n","    for key, value in dictionary.items():\n","        value_sentiment = value['true_sentiment']\n","        if value_sentiment in sentiment_counts and sentiment_counts[value_sentiment] > 0:\n","            copied_dict[key] = value\n","            sentiment_counts[value_sentiment] -= 1\n","        if all(count == 0 for count in sentiment_counts.values()):\n","            break\n","    return copied_dict\n","\n","def process_dataset(dataset, max_distribution, use_amazon=False):\n","    max_counts = check_dataset_distribution(dataset, max_distribution)\n","    print(\"\\n-------------------------------------------\")\n","    dataset = limit_dataset(dataset, max_counts, use_amazon)\n","    print(\"\\nTotal count: \", len(dataset))\n","    df = pd.DataFrame.from_dict(dataset, orient='index')\n","    unique_counts = df['true_sentiment'].value_counts()\n","    print(\"\\nModified distribution of values: \\n\", unique_counts)\n","    return dataset"],"metadata":{"id":"hGT8i1yfun7g","executionInfo":{"status":"ok","timestamp":1698355278129,"user_tz":-120,"elapsed":6,"user":{"displayName":"Szymski","userId":"12735740458565246672"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Process the Amazon dataset\n","dict_amazon_for_bert = process_dataset(dict_amazon_for_bert, 0.45, use_amazon=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-P_V8JB2ZI5","executionInfo":{"status":"ok","timestamp":1698355278693,"user_tz":-120,"elapsed":569,"user":{"displayName":"Szymski","userId":"12735740458565246672"}},"outputId":"f920d1cf-ab72-4915-fd45-54f2f4b1bc42"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Distribution of values: \n"," 5    80283\n","4    19377\n","1    11211\n","3    10317\n","2     6889\n","Name: true_sentiment, dtype: int64\n","\n","Total count:  57635.0\n","\n","Modified distribution of values: \n"," [36127.0, 8720.0, 5045.0, 4643.0, 3100.0]\n","\n","-------------------------------------------\n","\n","Total count:  57635\n","\n","Modified distribution of values: \n"," 5    36127\n","4     8720\n","3     5045\n","2     4643\n","1     3100\n","Name: true_sentiment, dtype: int64\n"]}]},{"cell_type":"code","source":["# Process the Twitter dataset\n","dict_twitter_for_bert = process_dataset(dict_twitter_for_bert, 0.75, use_amazon=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LEOpsjqk2c71","executionInfo":{"status":"ok","timestamp":1698355279800,"user_tz":-120,"elapsed":1111,"user":{"displayName":"Szymski","userId":"12735740458565246672"}},"outputId":"4a579959-fe02-4ba7-e6de-73a95ffc4bfb"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Distribution of values: \n"," 0    37977\n","1    37335\n","Name: true_sentiment, dtype: int64\n","\n","Total count:  56484.0\n","\n","Modified distribution of values: \n"," [28483.0, 28001.0]\n","\n","-------------------------------------------\n","\n","Total count:  56484\n","\n","Modified distribution of values: \n"," 1    28483\n","0    28001\n","Name: true_sentiment, dtype: int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"01LGK0L8QViM"},"source":["## BERT implementation for deciding on slang replacement"]},{"cell_type":"markdown","metadata":{"id":"7ZhSK01TQViN"},"source":["As already introduced in notebook 2a, in order to decide whether some word should be replaced with its translation or not its almost impossible to write a hard coded script that would not introduce errors in the text.\n","Thus, we decided to use BERT to decide whether to replace the slang word or not.\n","It should help use in cases where context matters so if we have a sentence like \"it is the best day of my life\". We should not replace the word \"it\" with \"information technology\" in the above stated sentences because it would not make sense context wise and grammar wise."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"BXtis7GVQViN","executionInfo":{"status":"ok","timestamp":1698355308628,"user_tz":-120,"elapsed":808,"user":{"displayName":"Szymski","userId":"12735740458565246672"}}},"outputs":[],"source":["class SlangBert():\n","    \"\"\"\n","    Model using pretrained Bert for picking the best candidate for slang word replacement.\n","    Model returns the word for the text with gap based on the highest probability.\n","    \"\"\"\n","    def __init__(self, bertmodel):\n","        self.use_cuda = torch.cuda.is_available()\n","        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n","        print(\"device: \", self.device)\n","        self.bertmodel = bertmodel\n","        # Initial tokenizer to tokenize the question later\n","        self.tokenizer = BertTokenizer.from_pretrained(self.bertmodel)\n","        self.model = BertForMaskedLM.from_pretrained(self.bertmodel).to(self.device)\n","         # We used pretrained BertForMaskedLM to fill in the blank, do not fine tuning so we set model to eval\n","        self.model.eval()\n","\n","    def get_score(self,question_tensors, segment_tensors, masked_index, candidate):\n","        # Tokenize the answer candidate\n","        # if isinstance(candidate, float): print(\"CANDIDATE: \", candidate)\n","        candidate_tokens = self.tokenizer.tokenize(str(candidate))\n","        # After tokenizing, we convert token to ids, (word to numerical)\n","        candidate_ids = self.tokenizer.convert_tokens_to_ids(candidate_tokens)\n","        predictions = self.model(question_tensors, segment_tensors)\n","        predictions_candidates = predictions[0,masked_index, candidate_ids].mean()\n","        return predictions_candidates.item()\n","\n","    def predict(self,row):\n","        if row['text_to_check'].count('___') > 1:\n","            raise ValueError(\"More than 1 ___ in sentence_to_check\")\n","        # Tokenizing questions, convert '___' to '_' so that we can MASK it\n","        question_tokens = self.tokenizer.tokenize(row['text_to_check'].replace('___', '_'))\n","        if len(question_tokens) > 500:\n","            # max is 512\n","            # print(\"Token indices sequence length is longer than the specified maximum sequence length for this BERT model\")\n","            return '', ''\n","        masked_index = question_tokens.index('_')\n","        # Assign [MASK] to blank that need to be completed\n","        question_tokens[masked_index] = '[MASK]'\n","        segment_ids = [0] * len(question_tokens)\n","        segment_tensors = torch.tensor([segment_ids]).to(self.device)\n","        question_ids = self.tokenizer.convert_tokens_to_ids(question_tokens)\n","        question_tensors = torch.tensor([question_ids]).to(self.device)\n","        candidates = row['candidates']\n","        # Return probabilities of answer choice in [...]\n","        predict_tensor = torch.tensor([self.get_score(question_tensors, segment_tensors,\n","                                                masked_index, candidate) for candidate in candidates])\n","        # Softmax the predict probability to return the index for maximum values\n","        predict_idx = torch.argmax(predict_tensor).item()\n","        return candidates[predict_idx], predict_tensor"]},{"cell_type":"markdown","metadata":{"id":"y0XCY6DKQViO"},"source":["## BERT processing of candidates\n","\n","The function **decide_on_translation** replaces slang/chat words with their corresponding translations only if the SlangModel coded above predicts to replace it. Otherwise, the slang/chat word will not be replaced.\n","\n","In Notebook 3 we predict the sentiment based on the final output of this notebook (replaced slang/chat words with its translation based on BERT's prediction) of a Amazon Product Review (Score: 1-5) or a tweet (Binary Target: 0,1) as well as with the inital data set, without any slang/chat word preprocessing."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"9yiYJ089QViO","executionInfo":{"status":"ok","timestamp":1698355310714,"user_tz":-120,"elapsed":3,"user":{"displayName":"Szymski","userId":"12735740458565246672"}}},"outputs":[],"source":["def decide_on_translation(dict_text_for_bert, use_progressive_updated_text, model):\n","    df_text_after_bert = pd.DataFrame(columns=['id','true_sentiment', 'text', 'candidates',\n","                                               'updated_candidates', 'processed_text',\n","                                               'is_same_as_original', 'chosen_translation'])\n","    dict_text_after_bert = {}\n","\n","    with tqdm(total=len(dict_text_for_bert)) as pbar:\n","        for index, row in dict_text_for_bert.items():\n","            text = row['text']\n","            index_shift = 0\n","            list_of_candidates_for_text = row['candidates'] # list of dicts\n","            updated_list_of_candidates_for_text = []\n","            updated_text_to_check = ''\n","            skip_this_row = False\n","            # print(\"------------------------------------------------------------------\\n\", \"FULL text: \", text)\n","            # process the candidates in order that was sorted by start_index\n","            for candidate in list_of_candidates_for_text:\n","                # print(\"\\tcandidate: \", candidate)\n","                if updated_text_to_check != '' and use_progressive_updated_text == True:\n","                    # temp = updated_text_to_check\n","                    updated_text_to_check = updated_text_to_check[:candidate['start_index']-index_shift] + '___' + updated_text_to_check[candidate['end_index']-index_shift:]\n","                    # if candidate['text_to_check'] != updated_text_to_check:\n","                    #     print('\\index_shift: ' + str(index_shift))\n","                    #     print('\\tupdated_text_to_check: ' + temp)\n","                    #     print(\"\\tCandidate OLD text_to_check: \", candidate['text_to_check'])\n","                    #     print(\"\\tCandidate NEW text_to_check: \", updated_text_to_check)\n","                    candidate['text_to_check'] = updated_text_to_check\n","\n","                my_dict = {'slang': candidate['slang_word'],\n","                            'candidates': candidate['list_of_translations'],\n","                            'text_to_check': candidate['text_to_check']}\n","                predicted_word, predict_tensor = model.predict(my_dict)\n","                if predicted_word == '' and predict_tensor == '':\n","                    skip_this_row = True\n","                    break\n","\n","                # Extract the highest and second-highest values and indices\n","                highest_values, _ = torch.topk(predict_tensor, k=2)\n","                difference = highest_values[0].item() - highest_values[1].item()\n","\n","                # # print the predictions\n","                # for word, tensor_value in zip(my_dict['candidates'], predict_tensor.tolist()):\n","                #     print(\"\\t-\", \"'\"+word+\"'\", tensor_value)\n","                # print(\"\\tPREDICTION: \", \"'\"+predicted_word+\"'\")\n","                # print(\"\\t------------------------------------------------------------------\\n\")\n","\n","                # update text and index_shift since start_index and end_index is always wrt to original text\n","                text = text[:candidate['start_index']-index_shift] + predicted_word + text[candidate['end_index']-index_shift:]\n","                # as long as slang_word == predicted_word, the index_shift will be the same\n","                index_shift = index_shift + len(candidate['slang_word']) - len(predicted_word)\n","                updated_text_to_check = text\n","\n","                # update candidate\n","                if predicted_word == candidate['slang_word']:\n","                    processed_candidate =  candidate.copy()\n","                    processed_candidate['chosen_translation'] = candidate['slang_word']\n","                    processed_candidate['was_replaced'] = False\n","                    processed_candidate['final_text'] = my_dict['text_to_check'].replace('___', candidate['slang_word'])\n","                    processed_candidate['diff'] = difference\n","                else:\n","                    processed_candidate = candidate.copy()\n","                    processed_candidate['chosen_translation'] = predicted_word\n","                    processed_candidate['was_replaced'] = True\n","                    processed_candidate['final_text'] = my_dict['text_to_check'].replace('___', predicted_word)\n","                    processed_candidate['diff'] = difference\n","                updated_list_of_candidates_for_text.append(processed_candidate)\n","\n","\n","            if skip_this_row: continue\n","            # append a row to df\n","            replacement_tuples = [(candidate['slang_word'], candidate['chosen_translation'], candidate['diff']) for candidate in updated_list_of_candidates_for_text if candidate['was_replaced']]\n","            df_text_after_bert.loc[len(df_text_after_bert)] = {\n","                                                                'id': index,\n","                                                                'true_sentiment': row['true_sentiment'],\n","                                                                'text': row['text'],\n","                                                                'candidates': row['candidates'],\n","                                                                'updated_candidates': updated_list_of_candidates_for_text,\n","                                                                'processed_text': text,\n","                                                                'is_same_as_original': text == row['text'],\n","                                                                'chosen_translation': replacement_tuples\n","                                                                }\n","            dict_text_after_bert[index] = {\n","                'true_sentiment': row['true_sentiment'],\n","                'text': row['text'],\n","                'candidates': row['candidates'],\n","                'updated_candidates': updated_list_of_candidates_for_text,\n","                'processed_text': text,\n","                'is_same_as_original': text == row['text'],\n","                'chosen_translation': replacement_tuples\n","\n","            }\n","            pbar.update(1)\n","    return df_text_after_bert, dict_text_after_bert"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"DE6fYFWqQViP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e31d1d72-14d1-4de8-92cd-37d865ed8690","executionInfo":{"status":"ok","timestamp":1698355413846,"user_tz":-120,"elapsed":100397,"user":{"displayName":"Szymski","userId":"12735740458565246672"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["device:  cuda\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 231508/231508 [00:00<00:00, 422212.35B/s]\n","100%|██████████| 1248501532/1248501532 [01:11<00:00, 17433440.50B/s]\n"]}],"source":["model = SlangBert('bert-large-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9VYdqu7hUPTK"},"outputs":[],"source":["# # execution for amazon\n","# df_amazon_after_bert, dict_amazon_after_bert = decide_on_translation(dict_amazon_for_bert, use_progressive_updated_text = use_bert_progressively, model = model)\n","\n","# # save df and dict\n","# df_amazon_after_bert.to_csv(path_data + 'BERT_data/'+ get_file_name('amazon') + '.csv', index=False)\n","# with open(path_data + 'BERT_data/'+ get_file_name('amazon') + '.pkl', 'wb') as handle:\n","#     pickle.dump(dict_amazon_after_bert, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"NNwV05dyUPTK","executionInfo":{"status":"ok","timestamp":1698360355667,"user_tz":-120,"elapsed":4941826,"user":{"displayName":"Szymski","userId":"12735740458565246672"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e27af58d-2b20-4757-8a56-688538de59eb"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 56484/56484 [1:22:19<00:00, 11.44it/s]\n"]}],"source":["# execution for twitter\n","df_twitter_after_bert, dict_twitter_after_bert = decide_on_translation(dict_twitter_for_bert, use_progressive_updated_text = use_bert_progressively, model = model)\n","\n","# save df and dict\n","df_twitter_after_bert.to_csv(path_data + 'BERT_data/'+ get_file_name('twitter') + '.csv', index=False)\n","with open(path_data + 'BERT_data/'+ get_file_name('twitter') + '.pkl', 'wb') as handle:\n","    pickle.dump(dict_twitter_after_bert, handle, protocol=pickle.HIGHEST_PROTOCOL)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"machine_shape":"hm","toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}